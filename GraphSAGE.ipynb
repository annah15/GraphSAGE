{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import TUDataset, Planetoid, Reddit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neighborhood sample size as described in experimental setup\n",
    "NEIGHBORHOOD_SAMPLE_SIZE = [25, 10]\n",
    "#Depth of neighborhood sampling and aggregation\n",
    "SEARCH_DEPTH = 2\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return dataset and data of input dataset name\n",
    "def get_data(dataset_name):\n",
    "    if(dataset_name == 'proteins'):\n",
    "        dataset = TUDataset(root=\"data\", name=\"PROTEINS\", use_node_attr=True)\n",
    "        data = dataset[0]\n",
    "        #TODO: correct mask implementation\n",
    "        masks = {'train' : torch.tensor([True for i in range(len(data))]),\n",
    "                 'val' : torch.tensor([True for i in range(len(data))]),\n",
    "                 'test' : torch.tensor([True for i in range(len(data))])}\n",
    "    elif(dataset_name == 'pubmed'):\n",
    "        dataset = Planetoid(root=\"data\", name=\"Pubmed\")\n",
    "        data = dataset[0]\n",
    "        masks = {'train' : data.train_mask,\n",
    "                 'val' : data.val_mask,\n",
    "                 'test' : data.test_mask}\n",
    "    else:\n",
    "        print(\"No\")\n",
    "\n",
    "    return dataset, data, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborLoader\n",
    "#implement Mini Batching with neighborhood sampling\n",
    "def neighborhood(data, input_nodes, batch_size):\n",
    "    return NeighborLoader(data, NEIGHBORHOOD_SAMPLE_SIZE, input_nodes=input_nodes, batch_size=batch_size, shuffle=True)\n",
    "#val_loader = NeighborLoader(data, NEIGHBORHOOD_SAMPLE_SIZE, input_nodes=pubmed_data.val_mask, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Dropout, Linear\n",
    "\n",
    "class SAGE(torch.nn.Module):\n",
    "    \"\"\" \n",
    "    GraphSAGE model\n",
    "    Params:\n",
    "        in_channels feature size of each input sample\n",
    "        hidden_channels feature size of each hidden sample\n",
    "        out_channel feature size of each output sample\n",
    "        num_layers =K number of message passing layers\n",
    "        aggregator_type (mean, pool, lstm)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, aggregator_type=\"mean\"):\n",
    "        super().__init__()\n",
    "        #SAGEConv layers\n",
    "        self.sage_in = SAGEConv(in_channels, hidden_channels, aggregator_type)\n",
    "        self.sage_out = SAGEConv(hidden_channels, out_channels, aggregator_type)\n",
    "        #Use Adam optimaizer vgl Experimental Setup\n",
    "        self.optimizer = torch.optim.Adam(self.parameters())\n",
    "        #Cross entropy loss for supervised learning TODO: unsupervised variant\n",
    "        self.loss_fn = F.cross_entropy\n",
    "    \n",
    "    \"\"\"\n",
    "    Forward Propagation\n",
    "    Params:\n",
    "        x input vector \n",
    "        edge_index adjacency matrix \n",
    "    \"\"\"\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.sage_in(x, edge_index)\n",
    "        h = h.relu_()\n",
    "        h = self.sage_out(h, edge_index)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, neighborhood, masks):\n",
    "    model.train()\n",
    "    #Use optimizer and lossfunktion of model\n",
    "    optimizer = model.optimizer\n",
    "    criterion = model.loss_fn\n",
    "\n",
    "    total_loss = total_correct = 0\n",
    "    for batch in neighborhood:\n",
    "        #Zero weights before calculating gradients\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch.x, batch.edge_index)\n",
    "        label = batch.y\n",
    "        #loss = criterion(output[batch.train_mask], label.squeeze(0)[batch.train_mask])\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_correct += torch.sum(output.argmax(dim=-1, keepdim=True) == label).item() / len(label)\n",
    "    \n",
    "    loss = total_loss / len(neighborhood)\n",
    "    acc = total_correct / len(neighborhood)\n",
    "    return loss, acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, mask):\n",
    "    model.eval()\n",
    "    criterion = model.loss_fn\n",
    "    output = model(data.x, data.edge_index)\n",
    "    label = data.y\n",
    "    loss = criterion(output[mask], label[mask])\n",
    "    total_loss = loss.item()\n",
    "    total_acc = torch.sum(output.argmax(dim=-1, keepdim=True) == label).item() * 1.0 / len(label)\n",
    "    return total_loss, total_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAGE(\n",
      "  (sage_in): SAGEConv(500, 250, aggr=mean)\n",
      "  (sage_out): SAGEConv(250, 3, aggr=mean)\n",
      ")\n",
      "Training...\n",
      "Epoch 00, Loss: 1.0977, Approx. Train: 174.4167\n",
      "Epoch 01, Loss: 1.0888, Approx. Train: 190.5107\n",
      "Epoch 02, Loss: 1.0795, Approx. Train: 203.0446\n",
      "Epoch 03, Loss: 1.0696, Approx. Train: 207.9081\n",
      "Epoch 04, Loss: 1.0579, Approx. Train: 192.9501\n",
      "Epoch 05, Loss: 1.0428, Approx. Train: 209.5797\n",
      "Epoch 06, Loss: 1.0283, Approx. Train: 194.5911\n",
      "Epoch 07, Loss: 1.0074, Approx. Train: 193.3956\n",
      "Epoch 08, Loss: 0.9843, Approx. Train: 208.4360\n",
      "Epoch 09, Loss: 0.9603, Approx. Train: 205.9044\n",
      "Epoch 10, Loss: 0.9384, Approx. Train: 194.8568\n",
      "Epoch 11, Loss: 0.9100, Approx. Train: 217.1437\n",
      "Epoch 12, Loss: 0.8833, Approx. Train: 215.6708\n",
      "Epoch 13, Loss: 0.8640, Approx. Train: 207.6046\n",
      "Epoch 14, Loss: 0.8306, Approx. Train: 213.5154\n",
      "Epoch 15, Loss: 0.8034, Approx. Train: 195.0369\n",
      "Epoch 16, Loss: 0.7813, Approx. Train: 213.5329\n",
      "Epoch 17, Loss: 0.7481, Approx. Train: 205.5117\n",
      "Epoch 18, Loss: 0.7210, Approx. Train: 217.1072\n",
      "Epoch 19, Loss: 0.7017, Approx. Train: 205.4173\n",
      "Epoch 20, Loss: 0.6757, Approx. Train: 214.8852\n",
      "Epoch 21, Loss: 0.6492, Approx. Train: 203.7148\n",
      "Epoch 22, Loss: 0.6158, Approx. Train: 193.9861\n",
      "Epoch 23, Loss: 0.6088, Approx. Train: 192.5456\n",
      "Epoch 24, Loss: 0.5919, Approx. Train: 196.3499\n",
      "Epoch 25, Loss: 0.5712, Approx. Train: 214.3508\n",
      "Epoch 26, Loss: 0.5498, Approx. Train: 198.9079\n",
      "Epoch 27, Loss: 0.5456, Approx. Train: 189.0108\n",
      "Epoch 28, Loss: 0.5192, Approx. Train: 195.9986\n",
      "Epoch 29, Loss: 0.5094, Approx. Train: 205.3676\n",
      "Epoch 30, Loss: 0.5051, Approx. Train: 199.4030\n",
      "Epoch 31, Loss: 0.4912, Approx. Train: 232.7830\n",
      "Epoch 32, Loss: 0.4701, Approx. Train: 200.6018\n",
      "Epoch 33, Loss: 0.4658, Approx. Train: 191.6331\n",
      "Epoch 34, Loss: 0.4537, Approx. Train: 202.8537\n",
      "Epoch 35, Loss: 0.4302, Approx. Train: 200.3584\n",
      "Epoch 36, Loss: 0.4096, Approx. Train: 215.3491\n",
      "Epoch 37, Loss: 0.4199, Approx. Train: 207.3594\n",
      "Epoch 38, Loss: 0.4083, Approx. Train: 205.6673\n",
      "Epoch 39, Loss: 0.4027, Approx. Train: 189.4238\n",
      "Epoch 40, Loss: 0.3892, Approx. Train: 219.0109\n",
      "Epoch 41, Loss: 0.3798, Approx. Train: 212.2748\n",
      "Epoch 42, Loss: 0.3921, Approx. Train: 201.2086\n",
      "Epoch 43, Loss: 0.3681, Approx. Train: 220.7336\n",
      "Epoch 44, Loss: 0.3548, Approx. Train: 199.4099\n",
      "Epoch 45, Loss: 0.3647, Approx. Train: 200.7622\n",
      "Epoch 46, Loss: 0.3604, Approx. Train: 230.7934\n",
      "Epoch 47, Loss: 0.3445, Approx. Train: 198.4212\n",
      "Epoch 48, Loss: 0.3385, Approx. Train: 188.7394\n",
      "Epoch 49, Loss: 0.3488, Approx. Train: 196.4865\n",
      "Epoch 50, Loss: 0.3340, Approx. Train: 200.5590\n",
      "Epoch 51, Loss: 0.3077, Approx. Train: 191.4205\n",
      "Epoch 52, Loss: 0.3160, Approx. Train: 206.3670\n",
      "Epoch 53, Loss: 0.3171, Approx. Train: 195.6852\n",
      "Epoch 54, Loss: 0.3157, Approx. Train: 213.8176\n",
      "Epoch 55, Loss: 0.2992, Approx. Train: 189.1391\n",
      "Epoch 56, Loss: 0.3069, Approx. Train: 201.2698\n",
      "Epoch 57, Loss: 0.3071, Approx. Train: 199.4489\n",
      "Epoch 58, Loss: 0.3090, Approx. Train: 220.7709\n",
      "Epoch 59, Loss: 0.2971, Approx. Train: 193.4740\n",
      "Epoch 60, Loss: 0.2846, Approx. Train: 198.0719\n",
      "Epoch 61, Loss: 0.2651, Approx. Train: 192.8159\n",
      "Epoch 62, Loss: 0.2915, Approx. Train: 197.4805\n",
      "Epoch 63, Loss: 0.2749, Approx. Train: 194.6238\n",
      "Epoch 64, Loss: 0.2820, Approx. Train: 213.8175\n",
      "Epoch 65, Loss: 0.2700, Approx. Train: 202.7619\n",
      "Epoch 66, Loss: 0.2695, Approx. Train: 210.7644\n",
      "Epoch 67, Loss: 0.2709, Approx. Train: 188.8973\n",
      "Epoch 68, Loss: 0.2703, Approx. Train: 192.2973\n",
      "Epoch 69, Loss: 0.2651, Approx. Train: 197.4046\n",
      "Epoch 70, Loss: 0.2603, Approx. Train: 190.4177\n",
      "Epoch 71, Loss: 0.2624, Approx. Train: 186.8370\n",
      "Epoch 72, Loss: 0.2557, Approx. Train: 192.4210\n",
      "Epoch 73, Loss: 0.2480, Approx. Train: 189.2524\n",
      "Epoch 74, Loss: 0.2475, Approx. Train: 185.9000\n",
      "Epoch 75, Loss: 0.2496, Approx. Train: 210.5051\n",
      "Epoch 76, Loss: 0.2368, Approx. Train: 203.9138\n",
      "Epoch 77, Loss: 0.2350, Approx. Train: 209.2833\n",
      "Epoch 78, Loss: 0.2491, Approx. Train: 223.6453\n",
      "Epoch 79, Loss: 0.2423, Approx. Train: 202.9456\n",
      "Epoch 80, Loss: 0.2381, Approx. Train: 195.7255\n",
      "Epoch 81, Loss: 0.2413, Approx. Train: 199.9021\n",
      "Epoch 82, Loss: 0.2033, Approx. Train: 198.5125\n",
      "Epoch 83, Loss: 0.2216, Approx. Train: 203.8566\n",
      "Epoch 84, Loss: 0.2251, Approx. Train: 204.5305\n",
      "Epoch 85, Loss: 0.2207, Approx. Train: 198.3529\n",
      "Epoch 86, Loss: 0.2111, Approx. Train: 206.1090\n",
      "Epoch 87, Loss: 0.2278, Approx. Train: 206.5149\n",
      "Epoch 88, Loss: 0.2211, Approx. Train: 205.7201\n",
      "Epoch 89, Loss: 0.2189, Approx. Train: 210.2283\n",
      "Epoch 90, Loss: 0.2161, Approx. Train: 189.8330\n",
      "Epoch 91, Loss: 0.2025, Approx. Train: 191.7738\n",
      "Epoch 92, Loss: 0.2126, Approx. Train: 189.5971\n",
      "Epoch 93, Loss: 0.2001, Approx. Train: 192.8653\n",
      "Epoch 94, Loss: 0.1956, Approx. Train: 197.6242\n",
      "Epoch 95, Loss: 0.1962, Approx. Train: 200.4382\n",
      "Epoch 96, Loss: 0.2088, Approx. Train: 212.7777\n",
      "Epoch 97, Loss: 0.1876, Approx. Train: 199.3069\n",
      "Epoch 98, Loss: 0.1914, Approx. Train: 208.6427\n",
      "Epoch 99, Loss: 0.1862, Approx. Train: 199.2747\n"
     ]
    }
   ],
   "source": [
    "dataset, data, masks = get_data('pubmed')\n",
    "\n",
    "model = SAGE(dataset.num_features, 250 ,dataset.num_classes)\n",
    "print(model)\n",
    "train_loader = neighborhood(data, masks['train'], 20)\n",
    "loss = np.empty(EPOCHS)\n",
    "new_loss = 0\n",
    "\n",
    "print(\"Training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    loss, acc = train(model, train_loader, masks)\n",
    "    print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Approx. Train: {acc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('GNN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f421deb9efadc0f436611e51624019c47b0612442c52c8cd7c27dd454d47f273"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
